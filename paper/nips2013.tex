\documentclass{article} % For LaTeX2e
\usepackage{nips13submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage[pdftex]{graphicx}
\usepackage{xcolor,colortbl}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{xspace}
\DeclareMathOperator*{\softmax}{softmax}
\DeclareMathOperator*{\LSTM}{LSTM}
\DeclareMathOperator*{\BiLSTM}{Bi-LSTM}
\DeclareMathOperator*{\HMN}{HMN}
\DeclareMathOperator*{\argmax}{argmax}
%\documentstyle[nips13submit_09,times,art10]{article} % For LaTeX 2.09
\newcommand{\dhid}{\ell}
\newcommand{\doclen}{m}
\newcommand{\queslen}{n}
\newcommand{\real}[1]{\mathbb{R}^{#1}}
\newcommand{\transpose}[1]{#1^{\top}}

\title{Question Answering System}


\author{
Kaushik Rangadurai\\
Software Engineer\\
Passage AI\\
\texttt{kaushikr@stanford.edu} \\
\And
Poorva Potdar \\
Software Engineer \\
Google Inc. \\
\texttt{poorvah@stanford.edu} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}

\maketitle

\begin{abstract}
In this paper, we have implemented and evaluated various attention mechanisms like Bidirectional Attention[1], Self-Attention[2] and Co-Attention[3]. We have also incorporated character embeddings in addition to the existing GloVe embeddings provided in the starter code and used a 1-dimensional Convolutional Neural Network (CNN) on top of the character embeddings. Our best performing model was able to get an EM of {\color{red}XX} and F1 of {\color{red}YY} on the test set.
\end{abstract}

\section{Introduction}

Most of the Natural Language Processing (NLP) problems can be seen as a question-answering problem. For example, a Search Engine (traditionally an Information Retrieval (IR) problem) can be seen as a Question-Answering problem - where the user typed query is the 'question', the document or a passage in a document is the answer. In fact, search engines like Google and Bing have recently introduced \textbf{answers} (a passage from the document) in the result page instead of just have document links. With the rise of personal assistants like Amazon Echo and Google Home and various chat-bots, the question answering system has become an important real-world problem to solve. From an end-user point of view, it gives a perception that a computer or a machine can not only understand humans but can also converse with them in natural language. To motivate this line of research, Stanford NLP group released the SQUAD[1] dataset, which consists of 100K question-answer pairs, along with a context paragraph for each pair.

The rest of the paper is organized as follows - section 2 defines the problem formally and section 3 analyzes the Squad dataset. Section 4 explains our model architecture by going over the math and in section 5, we go over some of the implementation details in Tensorflow. In section 6, we go over some of the ideas we wanted to implement but didn't due to the lack of time.

\section{Problem Definition}
\label{problem}

SQuAD is a reading comprehension dataset. This means that we will be given a paragraph and a question about that paragraph, as input. The goal is to answer the question correctly. For example, consider the following passage - 

\fbox{ 
  \parbox{\textwidth}{%
     \textbf{Question} Who proved that air is necessary for combustion? \\
      \textbf{Passage} In the late 17th century, Robert Boyle proved that air is necessary for combustion. English chemist John Mayow (1641?1679) refined this work by showing that fire requires only a part of air that he called spiritus 					nitroaereus or just nitroaereus. In one experiment he found that placing either a mouse or a lit candle in a closed container over water caused the water to rise and replace one-fourteenth of the air's volume before 				extinguishing the subjects. From this he surmised that nitroaereus is consumed in both respiration and combustion. \\
      \textbf{Answer} Robert Boyle \\
  }% 
}

It is important to note that for the SQuAD dataset, we don't have to generate text - we just need to point to the subtext of the passage that would contain the answer. As for metrics, we'll be looking at F1 and EM (Exact Match) to judge our answers. We'll look at them in detail in the section 6.

\section{Model}
\label{model}

\begin{figure}[t]
\centering
\includegraphics[width=34pc]{figures/model}
\caption{\small Our Model Architecture\space\space}
\label{fig:model}
\end{figure}

The architecture is split into the following layers - 

\begin{enumerate}
  \item \textbf{Character Embedding} - The character embedding layer maps each word to a higher dimensional space using a character level CNN. The characters are represented in a higher dimensional space and this is passed through a 1d CNN which is then max-pooled over the width to give the character embeddings.
  \item \textbf{Word Embedding} - We have used the word embeddings from GloVe. The embeddings were produced from a training dataset of 6 Billion tokens (Wikipedia and Gigaword) and has 400K unique words. We used the 100 dimensional word vectors for this paper.
  \item \textbf{Sequence 2 Sequence Layer} - The character embedding and the word embeddings are then concatenated and then passed to a Sequence to Sequence (Seq2Seq) model. This is a 1 layer Bidirectional LSTM model where the question is the encoder and the passage or context is the decoder. The hidden states from the forward pass and the backward pass are then concatenated and dropout is applied to avoid over-fitting.
  \item \textbf{Attention Layer} - After the Seq2Seq layer, we implemented various attention mechanisms. The starter code that was provided to us included the Basic Attention - a dot product of the hidden states of the encoder and the decoder to calculate the similarity. We also tried using the Multiplicative Attention, the Additive Attention, Bi-directional attention flow, Self-Attention and Co-Attention. As Co-Attention was the best performing model for us, we continued more experiments with Co-Attention.
  \item \textbf{Modeling Layer} - For the modeling layer, we used a simple fully-connected layer with ReLU non-linearity.
  \item \textbf{Output Layer} - For the output layer, we used a simple softmax layer to calculate the logits.
  \item \textbf{Prediction} - For the prediction, we used Dynamic Programming to calculate the maximum product of the probabilities of the start and end such that the index of the start is less than or equal to the index of the end.
\end{enumerate}

Below, we'll explain the theory behind Co-Attention - 

The Co-Attention network gets the inputs from the outputs of the Seq2Seq network which are both of dimenstions 2H where H is the dimension of the hidden layer. Assume that we've context hidden states $c_1...c_N \in \real{2H}$ and question hidden states $q_1...q_M \in \real{2H}$. From the hidden states of the question, we obtain a project question hidden states by applying a linear layer and tanh non-linearity.

\begin{center} $q_j^\prime = \tanh ( Wq_j + b ) \in \real{2H} \forall j \in \{1,...,M\} $. \end{center}

We now add a sentinel vector to both the context and question hidden states. We then compute the affinity matrix, which contains affinity scores corresponding to all pairs of document words and question words.

\begin{center} $L = \transpose{c_i} q_j^\prime \in \real{(n + 1) \times (m + 1)}$. \end{center}

We now use the affinity matrix to compute the attention in both the directions. The Context 2 Question attention (C2Q) \textbf{$a_i$} is calculated by :

\begin{center} $ \alpha^i = \softmax (L_{i:}) \in \real {(M+1)}$  \end{center}

\begin{center} $a_i = \sum_{j=1}^{M+1} \alpha_j^iq_j^\prime \in \real{2H}$  \end{center}

For the Q2C attention, we obtain the attention outputs \textbf{$b_j$} by - 


\begin{center} $\beta^j = \softmax (L_{:j}) \in \real {(N+1)}$  \end{center}

\begin{center} $b_j = \sum_{i=1}^{N+1} \beta_i^jc_i^\prime \in \real{2H}$  \end{center}

Next, we use the C2Q attention distributions $\alpha_i$ to take weighted sums of the Q2C attention outputs \textbf{$b_j$}. This gives us second-level attention outputs $s_i$ -

\begin{center} $s_i = \sum_{j=1}^{M+1} \alpha^i_j \beta_j \in \real{2H}$  \end{center}

Finally, we concatenate the second-level attention outputs $s_i$ with the first-level C2Q attention outputs $a_i$, and feed the sequence through a bidirectional LSTM. The resulting hidden states $u_i$ of this biLSTM are known as the coattention encoding. This is the overall output of the Coattention Layer.

\begin{center} $\{u_1, . . . ,u_N \} = biLSTM({[s_1; a_1], . . . , [s_N ; a_N ]})$  \end{center}

\section{Dataset}

The training dataset consists of 100k question-answer pairs with 97,554 unique words out of which 25,969 of them were not present in the GloVe vocabulary. Some of the most frequent words not present in the GloVe vocabulary were  'midna', 'asphalt/bitumen', 'pro-tibet', 'ganondorf' and other UTF-8 encoded words. This gave us the intuition to explore character embeddings and that it could perform better than a simple word embedding. There were a total of 174 unique characters in the dataset. Due to memory constraints, we picked the 70 most-frequent characters from the vocabulary and used this as our character vocabulary for the character embeddings. The remaining 70 characters characters were UTF-8 encoded.

\begin{table}[t]
\caption{Performance Comparison}
\label{performance-table}
\begin{center}
\begin{tabular}{ll}
\multicolumn{1}{c}{\bf Type of Question}  &\multicolumn{1}{c}{\bf Count}
\\ \hline \\
How	&10015 \\
When	&6399 \\
What		&48698 \\
Who	&9147 \\
Where	&3571 \\
Why	&1183 \\
Others	&7305 \\
\end{tabular}
\end{center}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=34pc]{figures/analysis}
\caption{\small Context and Question Length for Train and Dev Dataset \space\space}
\label{fig:analysis}
\end{figure}

From the figure ~\ref{fig:analysis}, we found that 99.99\% of the contexts had a length of 649 or below and 99.99\% of the questions had a length of 36 or below. Although we tried using the max length of the context and the question, due to memory constraints and faster iteration, we stuck to initial parameters of 600 for context length and 30 for question length.

\begin{figure}[t]
\centering
\includegraphics[width=34pc]{figures/start-end}
\caption{\small The start fraction and the end fraction in the Train Dataset \space\space}
\label{fig:startend}
\end{figure}

From the figure ~\ref{fig:startend}, We found that a lot of the start position of the answer was the beginning of the context and similarly a lot of the end position of the answer was the end of the context. This also led us to force a certain width between the start position and end position during prediction.

\section{Implementation Details}

This section lists the various hyper-parameters in our model. We had used a maximum word length of 16, a character embedding size of 20 and GloVe word embedding size of 100. We used a CNN with a window width of 5 and an output layer size of 100. For the Seq2Seq layer, the context was trimmed to a length of 600 words, the questions were were trimmed to a length of 30 words and the size of the hidden layer is 200.

\section{Results and Analysis}

\begin{table}[t]
\caption{Performance Comparison}
\label{performance-table}
\begin{center}
\begin{tabular}{lll}
\multicolumn{1}{c}{\bf Model}  &\multicolumn{1}{c}{\bf F1}	&\multicolumn{1}{c}{\bf EM}
\\ \hline \\
Baseline         &42.98	&33.77\\
Baseline with BiDaF Attention             &49.72	&39.87 \\
Co-Attention with GRU	&68	&58 \\
Co-Attention with LSTM	&69.30	&58.12 \\
Co-Attention with LSTM and Char Embedding	&70.09	&59.23 \\
Co-Attention with LSTM and CharCNN	&70.53	&59.36 \\
Co-Attention with LSTM and CharCNN and DP	&72.60	&60.20 \\
\end{tabular}
\end{center}
\end{table}

We've summarized our results in the Table ~\ref{table:performance-table}. We started with the baseline code that gave us an F1 measure of 42.98. As a first improvement, we substituted 
the Basic Attention in the Baseline with the BiDAF and this gave us an F1 of 49.72. We then substituted this with the Co-Attention with GRUs and this gave us a huge boost with an F1 of 68.
We explored 3 minor improvements on top of the Co-Attention - in the first model, we substituted the GRUs with LSTMs and this gave us 1 point increase. We then added character embeddings
(without the CNN) and this gave us another point increase and then added 1 layer of 1D CNN and this gave us a 0.5 point increase. We then used Dynamic Programming to intelligently choose and
end index such that end index is always greater than the start index and this gave us 2 point increase in the dev set. \\

We analyzed some examples of predicted answers by this model and below are some problems observed.

\textbf{Incorrect boundaries :}

Our model has a tendency to predict longer answers. Here is an example - 

\fbox{ 
  \parbox{\textwidth}{%
     \textbf{QUESTION:} when forces are acting on an extended body , what do you need to account for motion effects ? \\
     \textbf{TRUE ANSWER:} respective lines of application \\
     \textbf{PREDICTED ANSWER:} their respective lines of application must also be specified in order
     }%
 }

Although the predicted answer includes the expected answer, there are additional words in context that are included as well. We tried to alleviate this problem by trying to predict the answer end index to be within a window of size 'K'.
We incorporated this in the prediction. \\

\textbf{Redundant words with the question:} \\
Our model has a tendency to predict answers that have redundant words with the question. For example, below, the predicted answers could very well ignore the phrases like "per year" or "in general". \\

\fbox{ 
  \parbox{\textwidth}{%
	\textbf{QUESTION:} what did itv increase their yearly offer for control of the rights to broadcast the primer league to ? \\
	\textbf{TRUE ANSWER:} £34m \\
	\textbf{PREDICTED ANSWER:} £34m per year
}%
}

\fbox{ 
  \parbox{\textwidth}{%
     \textbf{QUESTION:} besides the study of prime numbers , what general theory was considered the official example of pure mathematics ? \\
      \textbf{TRUE ANSWER:} number theory \\
      \textbf{PREDICTED ANSWER:} number theory in general
      }%
}

\textbf{Attention does not distribute weights correctly:} \\
Sometimes, our models weighs certain parts of the context over the other, and ends up giving completely wrong answer.
In the example below, the attention layer should have picked "chen 's theorem", as we have C2Q and Q2C attentions. However, the model completely ignored the question.
We observed this problem, mostly for longer context > 150.

\fbox{ 
  \parbox{\textwidth}{%
   \textbf{CONTEXT:} in addition to the riemann hypothesis , many more conjectures revolving about primes have been posed . often having an elementary formulation , many of these conjectures have withst
ood a proof for decades : all four of landau 's problems from 1912 are still unsolved . one of them is goldbach 's conjecture , which asserts that every even integer
n greater than 2 can be written as a sum of two primes . as of february 2011 [ update ] , this conjecture has been verified for all numbers up to n = 2 · 1017 . weaker statements t
han this have been proven , for example vinogradov 's theorem says that every sufficiently large odd integer can be written as a sum of three primes . chen 's theorem says that every sufficiently large even number can be expressed as the sum of a prime and a semiprime , the product of two primes . also , any even integer can be written as the sum of six primes . the branch of number theory studying such questions is called additive number theory . \\
\textbf{QUESTION:} which theorem states that every large even integer can be written as a prime summed with a semiprime ? \\
\textbf{TRUE ANSWER:} chen 's theorem \\
\textbf{PREDICTED ANSWER:} goldbach 's conjecture
 }%
 }
 
\textbf{Human responses :} \\ 
Many times, our model does a good job at predicting the answer, but it does not match the human responses. \\

\fbox{ 
  \parbox{\textwidth}{%
   \textbf{QUESTION:} what division offers more then one branch of studies that do n't fit in with the other four ?\\
    \textbf{TRUE ANSWER:} the new collegiate division \\
    \textbf{PREDICTED ANSWER:} new collegiate division
    }%
}    

\section{Acknowledgments}

The authors would like to thank Richard Socher and the TAs for insightful discussions and timely feedback 
about improving the models. They also want to thank the classmates for creating a competitive and exciting leader
board.

\section{References}

\small{
[1] Min Joon Seo \& Aniruddha Kembhavi \& Ali Farhadi \& 
Hannaneh Hajishirzi (2016) Bidirectional Attention Flow 
for Machine Comprehension. arXiv preprint arXiv:1611.01603, 2016.

[2] Bhuwan Dhingra \& Hanxiao Liu \& Zhilin Yang \& William W Cohen
\& Ruslan Salakhutdinov. Gated-attention readers for text comprehension. 
arXiv preprint arXiv:1606.01549, 2016.

[3] Caiming Xiong \& Victor Zhong \& and Richard Socher. 
Dynamic coattention networks for question answering.
arXiv preprint arXiv:1611.01604, 2016.

[4] 
}

\end{document}
